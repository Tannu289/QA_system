# -*- coding: utf-8 -*-
"""QA_systemipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wkFCD1ifiQYKe3Ri8bYZB8BoC5SLdldV
"""

# Step 2: Loading and Exploring Custom QA Dataset from Google Drive

print("ğŸ”„ Mounting Google Drive and Loading Custom QA Dataset...")

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datasets import Dataset

# Load the CSV file
csv_path = "/content/drive/MyDrive/QA_system/train1.csv"

try:
    # Load CSV
    df = pd.read_csv(csv_path)
    print("âœ… CSV loaded successfully!")

    # Fix: wrap plain answers in {'text': [answer]}
    df['answers'] = df['answers'].apply(lambda x: {'text': [x] if pd.notna(x) else []})

    # Convert to Hugging Face Dataset
    dataset = Dataset.from_pandas(df)

    # Split into train and validation subsets
    train_dataset = dataset.select(range(min(5000, len(dataset))))
    validation_dataset = dataset.select(range(min(1000, len(dataset))))

except Exception as e:
    print(f"âŒ Error loading dataset: {e}")
    raise

# Display basic info
print(f"\nğŸ“Š Dataset Information:")
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(validation_dataset)}")
print(f"\nDataset features: {train_dataset.features}")

# Show sample examples
print(f"\nğŸ” Sample Examples:")
for i in range(min(3, len(train_dataset))):
    example = train_dataset[i]
    print(f"\nExample {i+1}:")
    print(f"Question: {example['query']}")
    print(f"Context: {example['finalpassage'][:200]}...")
    answer = example['answers']['text'][0] if example['answers']['text'] else 'No answer'
    print(f"Answer: {answer}")
    print("-" * 80)
# Basic statistics (skip entries with None)
valid_examples = [ex for ex in train_dataset if ex['query'] is not None and ex['finalpassage'] is not None]

questions_lengths = [len(ex['query'].split()) for ex in valid_examples]
context_lengths = [len(ex['finalpassage'].split()) for ex in valid_examples]

print(f"\nğŸ“ˆ Dataset Statistics:")
print(f"Average question length: {np.mean(questions_lengths):.1f} words")
print(f"Average context length: {np.mean(context_lengths):.1f} words")
print(f"Max question length: {max(questions_lengths)} words")
print(f"Max context length: {max(context_lengths)} words")

# Drop rows with missing query or context
df = df.dropna(subset=['query', 'finalpassage'])


# Visualize
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(questions_lengths, bins=30, alpha=0.7, color='blue')
plt.title('Question Length Distribution')
plt.xlabel('Number of words')
plt.ylabel('Frequency')

plt.subplot(1, 3, 2)
plt.hist(context_lengths, bins=30, alpha=0.7, color='green')
plt.title('Context Length Distribution')
plt.xlabel('Number of words')
plt.ylabel('Frequency')

plt.subplot(1, 3, 3)
# Answer availability
has_answer = [len(ex['answers']['text']) > 0 for ex in train_dataset]
answer_counts = pd.Series(has_answer).value_counts()
labels = ['Has Answer' if val else 'No Answer' for val in answer_counts.index]

# Safety check for pie chart
if len(answer_counts) < 2:
    print("âš ï¸ Not enough variety in answer presence to show a pie chart.")
else:
    plt.pie(answer_counts.values, labels=labels, autopct='%1.1f%%')
    plt.title('Answer Availability')


plt.tight_layout()
plt.show()

print("âœ… Dataset exploration completed!")
print("\nğŸ¯ Key Observations:")
print("- Custom dataset uses 'query' and 'finalpassage' columns")
print("- Dataset is ready for QA model training")

# âœ… Step 1: Install dependencies (run this only once in Colab)
# !pip install transformers torch

# âœ… Step 2: Import required libraries
import torch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline

# âœ… Step 3: Set device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… Step 4: Load model and tokenizer
print("ğŸ¤– Loading pretrained model and tokenizer...")

MODEL_NAME = "distilbert-base-uncased-distilled-squad"
print(f"Selected model: {MODEL_NAME}")

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)

    print("âœ… Model and tokenizer loaded successfully!")
    print(f"Model type: {type(model).__name__}")
    print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    exit()

# âœ… Step 5: Create QA pipeline
qa_pipeline = pipeline(
    "question-answering",
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

# âœ… Step 6: Test the pipeline with a sample
print("\nğŸ§ª Testing with a sample...")

test_context = """
The Amazon rainforest is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America.
This basin encompasses 7,000,000 square kilometers, of which 5,500,000 square kilometers are covered by the rainforest.
This region includes territory belonging to nine nations and 3,344 formally acknowledged indigenous territories.
"""
test_question = "How large is the Amazon basin?"

try:
    result = qa_pipeline(question=test_question, context=test_context)
    print(f"\nQuestion: {test_question}")
    print(f"Answer: '{result['answer']}'")
    print(f"Confidence: {result['score']:.4f}")
except Exception as e:
    print(f"âŒ Error during prediction: {e}")

# âœ… Step 7: Tokenizer Info
print("\nğŸ”¤ Tokenizer Info:")
print(f"Vocab size: {tokenizer.vocab_size}")
print(f"Max length: {tokenizer.model_max_length}")
print(f"Special tokens: {tokenizer.special_tokens_map}")

# âœ… Step 8: Tokenization Example
sample_text = "What is the capital of France?"
tokens = tokenizer.tokenize(sample_text)
token_ids = tokenizer.encode(sample_text)

print(f"\nTokenization Example:")
print(f"Text: {sample_text}")
print(f"Tokens: {tokens}")
print(f"Token IDs: {token_ids}")
print(f"Decoded: {tokenizer.decode(token_ids)}")

# âœ… Step 9: Model Architecture Overview
print(f"\nğŸ—ï¸ Model Architecture:\n{model}")

print("\nğŸ¯ Done: Model loaded, tested, and ready for fine-tuning!")

import torch
import matplotlib.pyplot as plt
from datasets import Dataset

# Configuration
MAX_LENGTH = 384
DOC_STRIDE = 128
MAX_QUERY_LENGTH = 64

# Check required columns
required_columns = {"query", "finalpassage", "answers"}
assert required_columns.issubset(dataset.column_names), f"Dataset must contain columns: {required_columns}"

print("ğŸ”„ Starting data preprocessing...")

# Preprocessing for training data
def preprocess_training_examples(examples):
    questions = [q.strip() if q else "" for q in examples["query"]]
    contexts = [c.strip() if c else "" for c in examples["finalpassage"]]

    tokenized_examples = tokenizer(
        questions,
        contexts,
        truncation="only_second",
        max_length=MAX_LENGTH,
        stride=DOC_STRIDE,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length"
    )

    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    offset_mapping = tokenized_examples.pop("offset_mapping")

    start_positions = []
    end_positions = []

    for i, offsets in enumerate(offset_mapping):
        input_ids = tokenized_examples["input_ids"][i]
        sequence_ids = tokenized_examples.sequence_ids(i)
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]

        # Safe access
        answer_starts = answers.get("answer_start", [])
        answer_texts = answers.get("text", [])

        if not answer_starts or not answer_texts:
            start_positions.append(0)
            end_positions.append(0)
            continue

        start_char = answer_starts[0]
        end_char = start_char + len(answer_texts[0])

        token_start_index = 0
        token_end_index = len(input_ids) - 1

        while sequence_ids[token_start_index] != 1:
            token_start_index += 1
        while sequence_ids[token_end_index] != 1:
            token_end_index -= 1

        answer_start = answer_end = None
        for idx in range(token_start_index, token_end_index + 1):
            start, end = offsets[idx]
            if start <= start_char < end:
                answer_start = idx
            if start < end_char <= end:
                answer_end = idx
                break

        if answer_start is None or answer_end is None:
            start_positions.append(0)
            end_positions.append(0)
        else:
            start_positions.append(answer_start)
            end_positions.append(answer_end)

    tokenized_examples["start_positions"] = start_positions
    tokenized_examples["end_positions"] = end_positions
    return tokenized_examples

# Preprocessing for validation data
def preprocess_validation_examples(examples):
    questions = [q.strip() if q else "" for q in examples["query"]]
    contexts = [c.strip() if c else "" for c in examples["finalpassage"]]

    tokenized_examples = tokenizer(
        questions,
        contexts,
        truncation="only_second",
        max_length=MAX_LENGTH,
        stride=DOC_STRIDE,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length"
    )

    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    tokenized_examples["example_id"] = [str(i) for i in sample_mapping]

    return tokenized_examples

# Apply preprocessing
print("ğŸ”„ Preprocessing training data...")
train_dataset = dataset.map(
    preprocess_training_examples,
    batched=True,
    remove_columns=dataset.column_names,
    desc="Preprocessing training data"
)

print("ğŸ”„ Preprocessing validation data...")
validation_dataset_processed = validation_dataset.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=validation_dataset.column_names,
    desc="Preprocessing validation data"
)

print("âœ… Preprocessing completed!")

# Summary
print(f"\nğŸ“Š Preprocessing Results:")
print(f"Original training examples: {len(dataset)}")
print(f"Processed training examples: {len(train_dataset)}")
print(f"Original validation examples: {len(validation_dataset)}")
print(f"Processed validation examples: {len(validation_dataset_processed)}")

# Sample output
print(f"\nğŸ” Sample Processed Example:")
sample = train_dataset[0]
print(f"Input IDs shape: {len(sample['input_ids'])}")
print(f"Start position: {sample['start_positions']}")
print(f"End position: {sample['end_positions']}")

decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=False)
print(f"\nDecoded text preview:\n{decoded_text[:200]}...")

if sample['start_positions'] > 0 and sample['end_positions'] > 0:
    answer_tokens = sample['input_ids'][sample['start_positions']:sample['end_positions'] + 1]
    predicted_answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)
    print(f"Extracted answer: '{predicted_answer}'")

# ğŸ“ˆ Visualization
start_positions = [ex['start_positions'] for ex in train_dataset]
end_positions = [ex['end_positions'] for ex in train_dataset]

answerable_count = sum(1 for s, e in zip(start_positions, end_positions) if s > 0 and e > 0)
unanswerable_count = len(start_positions) - answerable_count

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.hist(start_positions, bins=30, color='blue', alpha=0.7)
plt.title('Start Positions Distribution')
plt.xlabel('Token Index')
plt.ylabel('Count')

plt.subplot(1, 2, 2)
answer_lengths = [e - s for s, e in zip(start_positions, end_positions) if s > 0 and e > 0]
plt.hist(answer_lengths, bins=20, color='green', alpha=0.7)
plt.title('Answer Length Distribution')
plt.xlabel('Length')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

print("\nâœ… Step 4 completed successfully!")
print("ğŸ¯ Achievements:")
print("- Tokenized queries and passages")
print("- Computed answer start/end positions")
print("- Handled long passages with sliding window")
print("- Produced training-ready dataset ğŸ¯")

# Step 1: Imports
import torch
import numpy as np
from transformers import TrainingArguments, Trainer

# Assume these are already defined: model, tokenizer, train_dataset, validation_dataset_processed

# Step 2: Training hyperparameters
print("ğŸš€ Setting up training configuration...")

BATCH_SIZE = 8
LEARNING_RATE = 2e-5
NUM_EPOCHS = 2
WARMUP_STEPS = 100
WEIGHT_DECAY = 0.01
SAVE_STEPS = 500
LOGGING_STEPS = 100

# Step 3: TrainingArguments
training_args = TrainingArguments(
    output_dir='./qa_model_checkpoints',
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    weight_decay=WEIGHT_DECAY,
    warmup_steps=WARMUP_STEPS,
    logging_dir='./logs',
    logging_steps=LOGGING_STEPS,
    save_steps=SAVE_STEPS,
    save_total_limit=2,
    fp16=torch.cuda.is_available(),
    dataloader_pin_memory=False,
)

print("âœ… Training arguments configured!")
print(f"- Batch size: {BATCH_SIZE}")
print(f"- Learning rate: {LEARNING_RATE}")
print(f"- Epochs: {NUM_EPOCHS}")
print(f"- Mixed precision: {training_args.fp16}")

# Step 4: Custom data collator (FIXED)
class QADataCollator:
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def __call__(self, features):
        batch = {
            "input_ids": torch.stack([f["input_ids"] if isinstance(f["input_ids"], torch.Tensor) else torch.tensor(f["input_ids"]) for f in features]),
            "attention_mask": torch.stack([f["attention_mask"] if isinstance(f["attention_mask"], torch.Tensor) else torch.tensor(f["attention_mask"]) for f in features]),
        }
        if "start_positions" in features[0]:
            batch["start_positions"] = torch.tensor([f["start_positions"] for f in features], dtype=torch.long)
            batch["end_positions"] = torch.tensor([f["end_positions"] for f in features], dtype=torch.long)
        return batch

data_collator = QADataCollator(tokenizer)

# Step 5: Metrics
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    start_logits, end_logits = predictions
    start_preds = np.argmax(start_logits, axis=1)
    end_preds = np.argmax(end_logits, axis=1)
    start_true = labels[0]
    end_true = labels[1]
    return {
        "exact_match": np.mean((start_preds == start_true) & (end_preds == end_true)),
        "start_accuracy": np.mean(start_preds == start_true),
        "end_accuracy": np.mean(end_preds == end_true),
    }

# Step 6: Initialize Trainer
print("ğŸ”„ Initializing trainer...")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset_processed,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

print("âœ… Trainer initialized!")
print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(validation_dataset_processed)}")

# Step 7: Quick test
print("ğŸ§ª Testing trainer setup...")
try:
    test_batch = [train_dataset[i] for i in range(min(2, len(train_dataset)))]
    test_inputs = data_collator(test_batch)
    model.eval()
    with torch.no_grad():
        test_outputs = model(**test_inputs)
    print("âœ… Trainer setup test passed!")
    print(f"Start logits shape: {test_outputs.start_logits.shape}")
    print(f"End logits shape: {test_outputs.end_logits.shape}")
except Exception as e:
    print(f"âŒ Trainer setup test failed: {e}")

# Step 8: Train the model
print("ğŸ¯ Starting model training...")
if torch.cuda.is_available():
    torch.cuda.empty_cache()

try:
    training_output = trainer.train()
    print("ğŸ‰ Training completed!")
    print(f"Final training loss: {training_output.training_loss:.4f}")
    print(f"Training steps: {training_output.global_step}")

    print("ğŸ’¾ Saving model...")
    trainer.save_model("./final_qa_model")
    tokenizer.save_pretrained("./final_qa_model")
    print("âœ… Model saved.")
except Exception as e:
    print(f"âŒ Training failed: {e}")

print("âœ… Training pipeline complete.")

# Interactive Question-Answering System with Dataset Search

print("ğŸ” Starting Interactive Q&A System...")

# Load the trained model
try:
    if 'trainer' in locals():
        trained_model = trainer.model
        print("âœ… Using the just-trained model")
    else:
        trained_model = AutoModelForQuestionAnswering.from_pretrained("./final_qa_model")
        trained_model = trained_model.to(device)
        print("âœ… Loaded trained model from checkpoint")
except Exception as e:
    print(f"âš ï¸ Could not load trained model: {e}")
    print("Using the original pretrained model for demonstration")
    trained_model = model

# Create QA pipeline
print("ğŸ”„ Setting up Q&A pipeline...")
qa_pipeline = pipeline(
    "question-answering",
    model=trained_model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

# Function to search dataset for similar queries
def find_similar_queries(user_query, dataset, top_k=5):
    """
    Find the most similar queries from the dataset and return their answers.
    """
    similar_queries = []
    user_query_words = set(user_query.lower().split())

    print(f"ğŸ” Searching through {len(dataset)} queries for similar questions...")

    for i, example in enumerate(dataset):
        try:
            # Handle different possible data structures
            if isinstance(example, dict):
                dataset_query = example.get('query', '')
                answers = example.get('answers', '')
                context = example.get('finalpassage', '')
            elif isinstance(example, (list, tuple)) and len(example) >= 2:
                dataset_query = str(example[0]) if len(example) > 0 else ''
                answers = str(example[1]) if len(example) > 1 else ''
                context = str(example[2]) if len(example) > 2 else ''
            else:
                continue

            if not dataset_query:
                continue

            dataset_query_words = set(dataset_query.lower().split())

            # Calculate similarity score based on word overlap
            if len(user_query_words.union(dataset_query_words)) > 0:
                overlap_score = len(user_query_words.intersection(dataset_query_words)) / len(user_query_words.union(dataset_query_words))
            else:
                overlap_score = 0

            # Check for exact phrase matches
            phrase_bonus = 0
            user_query_lower = user_query.lower()
            dataset_query_lower = dataset_query.lower()

            if user_query_lower in dataset_query_lower or dataset_query_lower in user_query_lower:
                phrase_bonus = 0.5

            # Count common keywords
            keyword_matches = len(user_query_words.intersection(dataset_query_words))
            keyword_score = keyword_matches / max(len(user_query_words), 1)

            total_score = overlap_score + phrase_bonus + (keyword_score * 0.3)

            if total_score > 0.1:  # Only include reasonably similar queries
                similar_queries.append({
                    'dataset_query': dataset_query,
                    'answer': answers,
                    'context': context,
                    'score': total_score,
                    'index': i
                })

        except Exception as e:
            print(f"Error processing example {i}: {e}")
            continue

    # Sort by similarity score and return top_k
    similar_queries.sort(key=lambda x: x['score'], reverse=True)
    return similar_queries[:top_k]

# Function to answer user query by finding the best matching query in dataset
def answer_user_query(user_query, dataset, show_context=True):
    """
    Answer user's query by finding the most similar query in the dataset and returning its answer
    """
    print(f"\nğŸ” Processing query: '{user_query}'")
    print("=" * 80)

    # Find the best matching query in dataset
    similar_queries = find_similar_queries(user_query, dataset, top_k=1)

    if not similar_queries:
        print("âŒ No similar queries found in the dataset.")
        return None

    # Get the best match
    best_match = similar_queries[0]

    # Display result
    print(f"\nğŸ¯ BEST ANSWER FOR: '{user_query}'")
    print("=" * 80)

    print(f"ğŸ† Best Match (similarity: {best_match['score']:.3f})")
    print(f"   ğŸ“ Similar Question: '{best_match['dataset_query']}'")
    print(f"   ğŸ’¡ Answer: '{best_match['answer']}'")

    if show_context and best_match['context']:
        context_preview = best_match['context'][:200] + "..." if len(best_match['context']) > 200 else best_match['context']
        print(f"   ğŸ“– Context: {context_preview}")

    result = {
        'user_query': user_query,
        'matched_query': best_match['dataset_query'],
        'answer': best_match['answer'],
        'similarity_score': best_match['score'],
        'context': best_match['context']
    }

    return result

# Interactive query function
def interactive_qa_session(dataset):
    """
    Run an interactive Q&A session with the user
    """
    print("\nğŸ‰ Interactive Q&A System Ready!")
    print("=" * 50)
    print("Ask questions and I'll find similar queries in your dataset with their answers.")
    print("Type 'quit', 'exit', or 'stop' to end.")
    print("Type 'help' for more options.")
    print("=" * 50)

    session_history = []

    while True:
        try:
            user_input = input("\nğŸ’¬ Your question: ").strip()

            if not user_input:
                print("Please enter a question.")
                continue

            if user_input.lower() in ['quit', 'exit', 'stop', 'q']:
                print("ğŸ‘‹ Goodbye! Thanks for using the Q&A system.")
                break

            if user_input.lower() == 'help':
                print("\nğŸ“– Help Options:")
                print("- Ask any question and I'll find similar queries in your dataset")
                print("- Type 'sample' to see some example queries from your dataset")
                print("- Type 'history' to see previous questions")
                print("- Type 'stats' to see dataset statistics")
                print("- Type 'quit' to exit")
                continue

            if user_input.lower() == 'sample':
                print("\nğŸ“š Sample Queries from Dataset:")
                sample_count = 0
                for example in dataset[:10]:
                    try:
                        if isinstance(example, dict) and 'query' in example and example['query']:
                            print(f"   â€¢ {example['query']}")
                            sample_count += 1
                        elif isinstance(example, (list, tuple)) and len(example) > 0:
                            print(f"   â€¢ {str(example[0])}")
                            sample_count += 1
                        if sample_count >= 5:
                            break
                    except:
                        continue
                if sample_count == 0:
                    print("   No sample queries found in dataset structure")
                continue

            if user_input.lower() == 'history':
                if session_history:
                    print("\nğŸ“š Session History:")
                    for i, (q, a) in enumerate(session_history, 1):
                        print(f"{i}. Q: {q}")
                        print(f"   A: {a}")
                else:
                    print("No questions asked yet in this session.")
                continue

            if user_input.lower() == 'stats':
                print(f"\nğŸ“Š Dataset Statistics:")
                print(f"Total examples: {len(dataset)}")

                # Try to count valid queries
                valid_queries = 0
                for example in dataset:
                    try:
                        if isinstance(example, dict) and 'query' in example and example['query']:
                            valid_queries += 1
                        elif isinstance(example, (list, tuple)) and len(example) > 0 and str(example[0]).strip():
                            valid_queries += 1
                    except:
                        continue
                print(f"Valid queries found: {valid_queries}")
                continue

            # Answer the user's question
            result = answer_user_query(user_input, dataset, show_context=True)

            # Store in history
            if result:
                session_history.append((user_input, result['answer']))

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Session interrupted. Goodbye!")
            break
        except Exception as e:
            print(f"âŒ An error occurred: {e}")
            print("Please try asking your question again.")

# Quick evaluation function
def quick_evaluation(dataset):
    """
    Quick validation of dataset structure without showing examples
    """
    print(f"\nğŸ”¬ Validating Dataset Structure...")
    print("=" * 60)

    valid_count = 0
    for i, example in enumerate(dataset):
        try:
            # Handle different data structures
            if isinstance(example, dict):
                query = example.get('query', '')
                answer = example.get('answers', '')
                if query and str(query).strip():
                    valid_count += 1
            elif isinstance(example, (list, tuple)) and len(example) >= 2:
                query = str(example[0]) if len(example) > 0 else ''
                if query and query.strip():
                    valid_count += 1
        except Exception as e:
            continue

    print(f"ğŸ“Š Dataset validation complete:")
    print(f"   Total examples: {len(dataset)}")
    print(f"   Valid query-answer pairs: {valid_count}")

    if valid_count == 0:
        print("âŒ Could not find any valid query-answer pairs in the dataset.")
        print("Please check your dataset structure.")
    else:
        print(f"âœ… Dataset ready for querying!")

# Main execution
if __name__ == "__main__":
    # Check if dataset is available
    try:
        # Assuming your dataset is loaded in a variable (adjust variable name as needed)
        if 'validation_dataset' in locals():
            dataset = validation_dataset
        elif 'train_dataset' in locals():
            dataset = train_dataset
        elif 'dataset' in locals():
            dataset = dataset
        else:
            print("âŒ No dataset found. Please ensure your dataset is loaded.")
            print("Expected variables: 'validation_dataset', 'train_dataset', or 'dataset'")
            exit()

        print(f"âœ… Dataset loaded with {len(dataset)} examples")

        # Run quick evaluation first
        quick_evaluation(dataset)

        # Start interactive session
        interactive_qa_session(dataset)

    except Exception as e:
        print(f"âŒ Error: {e}")
        print("Please ensure your dataset is properly loaded before running this script.")

print("\nâœ… Interactive Q&A System completed!")